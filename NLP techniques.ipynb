{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.',\n",
       " 'Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.',\n",
       " 'It becomes vital to understand the pattern in the text to achieve the above-stated purpose.',\n",
       " 'These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\\\n",
    "    Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve the above-stated purpose. These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization.\"\n",
    "\n",
    "# tokenize sentence - Split the paragraph into sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'by',\n",
       " 'which',\n",
       " 'big',\n",
       " 'quantity',\n",
       " 'of',\n",
       " 'text',\n",
       " 'is',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'parts',\n",
       " 'called',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'used',\n",
       " 'for',\n",
       " 'building',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Text',\n",
       " 'classification',\n",
       " ',',\n",
       " 'intelligent',\n",
       " 'chatbot',\n",
       " ',',\n",
       " 'sentimental',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'language',\n",
       " 'translation',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'It',\n",
       " 'becomes',\n",
       " 'vital',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'pattern',\n",
       " 'in',\n",
       " 'the',\n",
       " 'text',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'the',\n",
       " 'above-stated',\n",
       " 'purpose',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " 'are',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'finding',\n",
       " 'such',\n",
       " 'patterns',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'is',\n",
       " 'considered',\n",
       " 'as',\n",
       " 'a',\n",
       " 'base',\n",
       " 'step',\n",
       " 'for',\n",
       " 'stemming',\n",
       " 'and',\n",
       " 'lemmatization',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize words - Split the sentences into words\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "increas\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"playing\"))\n",
    "print(stemmer.stem(\"plays\"))\n",
    "print(stemmer.stem(\"played\"))\n",
    "print(stemmer.stem(\"increase\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Resh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increases\n",
      "running\n",
      "run\n",
      "controlling\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(lemm.lemmatize(\"Increases\"))\n",
    "print(lemm.lemmatize(\"running\"))\n",
    "\n",
    "print(lemm.lemmatize(\"running\", pos=\"v\"))\n",
    "\n",
    "print(lemm.lemmatize(\"controlling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Resh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokenization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('big', 'JJ'),\n",
       " ('quantity', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('divided', 'VBN'),\n",
       " ('into', 'IN'),\n",
       " ('smaller', 'JJR'),\n",
       " ('parts', 'NNS'),\n",
       " ('called', 'VBD'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('building', 'VBG'),\n",
       " ('applications', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('Text', 'NNP'),\n",
       " ('classification', 'NN'),\n",
       " (',', ','),\n",
       " ('intelligent', 'JJ'),\n",
       " ('chatbot', 'NN'),\n",
       " (',', ','),\n",
       " ('sentimental', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " (',', ','),\n",
       " ('language', 'NN'),\n",
       " ('translation', 'NN'),\n",
       " (',', ','),\n",
       " ('etc', 'FW'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('becomes', 'VBZ'),\n",
       " ('vital', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('understand', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('pattern', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('achieve', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('above-stated', 'JJ'),\n",
       " ('purpose', 'NN'),\n",
       " ('.', '.'),\n",
       " ('These', 'DT'),\n",
       " ('tokens', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('very', 'RB'),\n",
       " ('useful', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('finding', 'VBG'),\n",
       " ('such', 'JJ'),\n",
       " ('patterns', 'NNS'),\n",
       " ('as', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('is', 'VBZ'),\n",
       " ('considered', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('base', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('stemming', 'VBG'),\n",
       " ('and', 'CC'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# get synonyms\n",
    "wordnet.synsets('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('control.v.01'),\n",
       " Synset('control.v.02'),\n",
       " Synset('operate.v.03'),\n",
       " Synset('manipulate.v.05'),\n",
       " Synset('control.v.05'),\n",
       " Synset('control.v.06'),\n",
       " Synset('see.v.10'),\n",
       " Synset('master.v.04'),\n",
       " Synset('controlling.s.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get synonyms\n",
    "wordnet.synsets('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'to')\n",
      "('to', 'play')\n",
      "('play', 'football')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"I love to play football\"\n",
    "\n",
    "n=2\n",
    "for gram in ngrams(word_tokenize(sentence), n):\n",
    "    print (gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
